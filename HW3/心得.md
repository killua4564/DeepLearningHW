# HW3 心得
## 檔案說明
- owo1.py HW3-1 build word2vec models
- owo2.py HW3-2
- owo3.py remove 沒意義的data
- owo4.py reload model to predict
## word2vec
- size
    - 詞向量的維度大小
    - 之前印象老師上課講是270 所以就用了
- iter
    - 訓練的回數
    - 之前都沒注意到這個 後來都固定用10 效果有好一些
- sg
    - sg=0時以CBOW來訓練，sg=1時以Skip-gram來訓練
    - 這個之前也都沒注意到，後來看第一名的似乎推薦用1我就跟著用了
- min_count
    - 只收入出現次數大於等於min_count的詞
    - 之前都用1 每次training都6小時起跳 用2或以上就舒暢多了 雖然我把model生到10來比較總length 但最多也只有試到3而已
- workers
    - 訓練的線程數
    - 之前沒注意到 後來固定用10來加速
## pre-processing
這部分我搞了快3天，本來是相信著講義做，一直在想辦法解決，結果同學用一些根本沒提過的東西，才可以正常的train，讓我有點傻眼
- Tokenizer
    - fit_on_texts
        - 把輸入的值去重後用index代入
    - texts_to_sequences
        - 把輸入的字用之前建好的index取代
    - pad_sequences
        - 修成固定長度的list 以符合Embedding Layer的條件
    - word_index
        - 列出所有的word和index的對應關係
另外我有對raw_data去除一些沒意義的字
e.g. a href、span、XD 等等
## model
- 第一層Embedding Layer
- 第二層LSTM / SimpleRNN
- 第三層GRU / Dense
- 第四層SimpleRNN / Dense
- 第五層Dense
    - 第一層的不用說太多，就直接對應上面設好的參數
    - 第二層原本用SimpleRNN先試run，後來都改用LSTM
    - 第二三四層測起來感覺沒什麼太大的差別，只是感覺很多RNN就比較厲害而已，比較有感覺的是activation，經驗上是tanh、relu、tanh交錯，然後RNN接RNN記得return_sequences=True
    - 第五層Dense參數units=10、activation=softmax，沒什麼太大特別
- kernel_initializer
    - 個人習慣用RandomUniform，主要是控制上下界和種子
- loss
    - 除了categorical_crossentropy好像沒有什麼其他特別可以用
- optimizer
    - 個人習慣用Adam，主要是可以自己調learning rate
- learning rate
    - 這個是說，有一天我在解issue，餓著肚子de了快1小時，後來終於受不了，跑去sukiya吃大碗的咖喱飯，回來之後bug就自己消失了
    - 他default是.001，然後我加上咖哩的祝福2626(咖哩咖哩)，就變成了.0012626，train起來結果都有在進步，所以就一直沿用了
- batch_size
    - 之前為了加快train的速度，有先調到100，後來穩定之後就條回20了
- validation_split
    - 個人習慣用.3，我覺得fit的val_loss、val_acc比較重要