result	

index	loss			acc				val_loss		val_acc
1		2.282952043745	0.152222222752	2.250169983617	0.219259259877
2		2.206203858058	0.247619046105	2.138574255837	0.276296293294
3		2.061681720946	0.314761906862	1.952145938520	0.331111116542
4		1.869522770246	0.356031742361	1.761527114444	0.382222225269
5		1.663908799489	0.425555553701	1.565787761300	0.455555555997
6		1.439531313048	0.508253975047	1.352619436052	0.521851846465
7		1.217626372973	0.580317450894	1.149559263830	0.605925917625
8		1.023103932540	0.662222219838	0.979088628734	0.676666661545
9		0.854369302591	0.734761913617	0.845156972055	0.721851847790
10		0.723850462172	0.780952374140	0.736741072602	0.760740750366
11		0.627422716882	0.804920633634	0.652723436002	0.788148142673
12		0.560378859440	0.822857141495	0.616127643320	0.801851848761
13		0.501124276055	0.843174603250	0.563576340675	0.816296314752
14		0.454977134864	0.858253969087	0.537639339765	0.829629624331
15		0.422653834025	0.867936511834	0.520243933907	0.835925925661
16		0.397481722964	0.874761905935	0.502791589057	0.840000009095
17		0.371510995759	0.883809520139	0.495208116593	0.847407405023
18		0.355689479245	0.887619051668	0.485870886732	0.845185187128
19		0.346845984459	0.889047609435	0.475434758045	0.848518530528
20		0.320696873797	0.898571418391	0.470530415023	0.855925924248
21		0.305734664202	0.904444449478	0.458727641238	0.854444446387
22		0.295945814914	0.907460325294	0.458116643959	0.855555549816
23		0.282387273179	0.912063492669	0.440812635201	0.860740725641
24		0.270814220111	0.917619042926	0.436914626095	0.864444434643
25		0.261782522003	0.919047613939	0.445126228862	0.866296302389
26		0.252772683899	0.924761904611	0.427149398459	0.869259266942
27		0.242959716254	0.926984124713	0.430315810221	0.868888892509
28		0.235410933693	0.927301592297	0.421645684375	0.872962980359
29		0.225046970778	0.933492064476	0.419931647954	0.872222222664
30		0.214980837372	0.938412692812	0.425243613897	0.870000013599
31		0.212394586868	0.935873011748	0.423123247094	0.870740735972
32		0.207207557228	0.939523796240	0.413799495609	0.875925918420
33		0.194453712967	0.943968249692	0.417638156149	0.874444431729
34		0.191367298365	0.942698419094	0.422674611763	0.873333339338
35		0.183249233498	0.945238100158	0.408205137209	0.879629638460
36		0.173951948682	0.950476196077	0.405118672936	0.878888891803
37		0.166409110030	0.951269845168	0.407173805767	0.882222230788
38		0.157632335193	0.957460317347	0.406175717160	0.881111109698
39		0.152547775043	0.955714278751	0.416980589981	0.879629638460
40		0.146192141705	0.958253973060	0.409883656987	0.882592591974
41		0.141860781444	0.960634926955	0.421750523426	0.883333340839
42		0.139783331917	0.958253973060	0.417715126718	0.883703697611
43		0.131540921827	0.963015874227	0.415592082121	0.884444442060
44		0.124263593720	0.964603172408	0.421431522678	0.885555554319
45		0.117465230326	0.966507944796	0.428207703211	0.882592594182
46		0.114342331886	0.968095242977	0.431659240414	0.885185199755
47		0.107937961817	0.970158729288	0.429721592753	0.884444455306
48		0.102898488442	0.972063488430	0.429338717902	0.886666682031
49		0.098394696911	0.973492066065	0.433885045626	0.884814807662
50		0.096115785340	0.972698416975	0.442786892255	0.886666664371
51		0.090456092523	0.975873019960	0.442604508665	0.886666670994
52		0.084904954665	0.978888889154	0.445927244646	0.889259265529
53		0.080767977983	0.978571428193	0.460474871927	0.885925926544
54		0.076883998596	0.980317466789	0.458753807677	0.885555556527
55		0.074022534821	0.981428576840	0.463781683533	0.887777756762
56		0.069007377658	0.982857134607	0.467194208392	0.887037041011
57		0.065802703301	0.984761913617	0.478204356300	0.886666668786
58		0.063791721645	0.985079354710	0.484459743456	0.888518525494
59		0.059614633107	0.986190471384	0.494985819967	0.883333332009
60		0.057627030131	0.986349198553	0.488870065521	0.887777765592
61		0.054276765221	0.988888879617	0.502716838210	0.887407404405
62		0.054384901292	0.987460308605	0.524469866797	0.881481479715
63		0.059694283952	0.985079354710	0.520157668326	0.881481484131
64		0.052822798491	0.987460315228	0.519351204236	0.887037027765
65		0.047679101841	0.989841255877	0.520364664219	0.884814829738
66		0.043394688103	0.991587301095	0.522663933259	0.887407408820
67		0.040649172333	0.991746028264	0.529488774361	0.888888873436
68		0.038047108385	0.992857144939	0.537141047142	0.887037041011
69		0.035892531276	0.993492066860	0.549791400079	0.887037036596
70		0.035040604778	0.993809514576	0.553181908749	0.887777783253
71		0.034418937026	0.993333339691	0.557289129054	0.886666655540
72		0.037465945714	0.992380950186	0.586910378050	0.881111111906
73		0.036643511926	0.992063482602	0.573699308766	0.884074069836
74		0.030432930837	0.994920637872	0.567799812114	0.885555565357
75		0.029653739598	0.995079371664	0.572130303692	0.885925933167
76		0.028508805773	0.994603176912	0.585533477642	0.885555556527
77		0.026180747069	0.995873027378	0.585722347101	0.884444444268
78		0.023377714679	0.996507942677	0.595264893991	0.883703695403
79		0.022043697329	0.996349215508	0.598764761730	0.886296281108
80		0.021838864105	0.996666676468	0.622726471336	0.885185171057
81		0.020664659329	0.996984137429	0.623273348367	0.884074067628
82		0.019691226487	0.997142871221	0.627101518490	0.882592598597
83		0.018856466437	0.997460332182	0.630652785301	0.885925922129
84		0.017434115315	0.997301591767	0.637418718250	0.885185193132
85		0.016262736792	0.997460325559	0.638159440623	0.885185193132
86		0.015602516010	0.997460325559	0.643082775452	0.884444459721
87		0.014899046988	0.997460325559	0.649308255425	0.885555549904
88		0.014291975337	0.997619059351	0.659066368032	0.884814809870
89		0.013642808836	0.997777786520	0.663520066826	0.883703713064
90		0.013075387519	0.997619065973	0.667660982520	0.885185186510
91		0.012569371714	0.997777786520	0.677359938622	0.884814816493
92		0.012186668296	0.997777786520	0.684455037117	0.885925922129
93		0.011441289965	0.997777786520	0.683640095923	0.884074058798
94		0.011112350867	0.997936513689	0.685767593207	0.884074074251
95		0.010534809540	0.998095254103	0.694643482014	0.885555552112
96		0.010282342943	0.998095254103	0.699095904827	0.885185179887
97		0.010053699215	0.998095254103	0.705007312474	0.884814816493
98		0.009342099799	0.998571435610	0.705872078737	0.884074067628
99		0.008970562110	0.998571435610	0.717407447320	0.884074069836
100		0.008676793530	0.998571435610	0.719233934526	0.885185188717

max_length = 500
min_count=2, size=270, iter=10, sg=1, workers=10

Embedding(input_dim=len(word_index)+1, output_dim=word2Vec.vector_size, weights=[embedding_matrix], input_length=max_length, trainable=False)
GRU(units=16, kernel_initializer=initializers.glorot_uniform(seed=1), activation='tanh')
Dense(units=100, kernel_initializer=initializers.glorot_uniform(seed=1), activation='relu')
Dense(units=100, kernel_initializer=initializers.glorot_uniform(seed=1), activation='relu')
Dense(units=10, kernel_initializer=initializers.glorot_uniform(seed=1), activation='softmax')

optimizer=optimizers.Adam(lr=.00125252100)

model.fit(x=train_x, y=train_y, epochs=100, validation_split=.3, batch_size=700, verbose=1)
