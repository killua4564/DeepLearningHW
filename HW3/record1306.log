result	0.88800

index	loss			acc				val_loss		val_acc
1		2.299678705357	0.117901236371	2.283800363541	0.159999996424
2		2.280728702192	0.179876539442	2.264677762985	0.218888893723
3		2.261102446803	0.242098766345	2.243011236191	0.267777770758
4		2.237829852987	0.280123458968	2.216768503189	0.295555561781
5		2.208705239826	0.298518513088	2.184209346771	0.302222222090
6		2.172384659449	0.310864203506	2.144563198090	0.313333332539
7		2.128994517856	0.334197528936	2.097551822662	0.322222232819
8		2.078051355150	0.342716051473	2.045425653458	0.328888893127
9		2.021890101609	0.350493832871	1.991855382919	0.332222223282
10		1.961481787540	0.363086423388	1.937693715096	0.342222213745
11		1.900990053459	0.372098768199	1.884335398674	0.346666663885
12		1.840170590966	0.386172842096	1.830625534058	0.366666674614
13		1.781459914313	0.402839496180	1.776508331299	0.385555565357
14		1.722197885867	0.418888876836	1.724264860153	0.394444435835
15		1.662480773749	0.434814815168	1.669100165367	0.412222236395
16		1.600817967344	0.455308631614	1.610226869583	0.423333346844
17		1.534808264838	0.475555564518	1.546928286552	0.447777777910
18		1.465355537556	0.498518508893	1.476696372032	0.465555548668
19		1.391404946645	0.525185174412	1.402932167053	0.493333339691
20		1.313218209479	0.554814808899	1.323377013206	0.523333311081
21		1.232599960433	0.580493807793	1.239511013031	0.571111083031
22		1.153730198189	0.612592575727	1.154691576958	0.589999973774
23		1.077673519099	0.640246918908	1.082161903381	0.622222244740
24		1.007202241156	0.670617286806	1.014967560768	0.656666696072
25		0.945040415834	0.688888894187	0.954777300358	0.685555577278
26		0.888761268722	0.717901238689	0.903144180775	0.714444458485
27		0.840097495803	0.736172841655	0.852882146835	0.721111118793
28		0.790872569437	0.751851852293	0.804360687733	0.747777760029
29		0.748629219002	0.767901226326	0.764984071255	0.761111140251
30		0.708217821739	0.782345685694	0.729919791222	0.759999990463
31		0.672903718772	0.796543231717	0.696663796902	0.778888881207
32		0.640450146463	0.803580248797	0.669877767563	0.782222211361
33		0.611084500949	0.812716044762	0.640501379967	0.788888871670
34		0.585417910858	0.820000010508	0.618013441563	0.800000011921
35		0.560715403822	0.826790138527	0.599494338036	0.797777771950
36		0.537925018205	0.831975296692	0.580534219742	0.804444432259
37		0.519548376401	0.839259266853	0.563582301140	0.808888912201
38		0.501296120661	0.846543219354	0.540761709213	0.812222242355
39		0.482125838598	0.849753084006	0.534548163414	0.824444472790
40		0.466286525682	0.856049376505	0.512808203697	0.822222232819
41		0.454174758108	0.860740758755	0.515211224556	0.828888893127
42		0.438219221654	0.864197521298	0.488847732544	0.828888893127
43		0.427644912843	0.868271609147	0.490559697151	0.838888883591
44		0.413555528279	0.874938291532	0.473086506128	0.844444453716
45		0.401415949618	0.878271601818	0.471622258425	0.841111123562
46		0.391089515554	0.881975317443	0.456192523241	0.847777783871
47		0.380623275483	0.884691359820	0.457987427711	0.851111114025
48		0.371715193545	0.887777774422	0.440770834684	0.846666693687
49		0.362053447300	0.888765427801	0.444473117590	0.854444444180
50		0.353811482588	0.892469143426	0.427599281073	0.850000023842
51		0.345828160092	0.893333326887	0.426744937897	0.858888864517
52		0.337917471374	0.896296280402	0.418035894632	0.864444434643
53		0.330074098375	0.898641970423	0.412339746952	0.863333344460
54		0.322863329340	0.902098774910	0.407738626003	0.865555584431
55		0.315267599291	0.905679013994	0.404344797134	0.866666674614
56		0.309096822032	0.906419756236	0.395753681660	0.867777764797
57		0.302896087920	0.907407416238	0.396531045437	0.868888914585
58		0.297407462641	0.909753081975	0.389706790447	0.868888914585
59		0.291069770301	0.913086407714	0.390973955393	0.867777764797
60		0.285485304064	0.913827158787	0.378815859556	0.875555574894
61		0.280618718377	0.914691357701	0.383453071117	0.873333334923
62		0.274756913936	0.916419755529	0.373435974121	0.874444425106
63		0.269476114600	0.918888902223	0.374876469374	0.877777755260
64		0.264724118842	0.919506165716	0.370819240808	0.876666665077
65		0.258787298644	0.921975303579	0.369176715612	0.878888905048
66		0.255037553332	0.922839522362	0.363521248102	0.878888905048
67		0.250467339048	0.924321004638	0.364289075136	0.882222235203
68		0.245770974292	0.925925923718	0.362544178963	0.883333325386
69		0.241465592826	0.927160481612	0.357943475246	0.883333325386
70		0.237675475302	0.928518516046	0.357056617737	0.885555565357
71		0.232785258028	0.930123439542	0.355307072401	0.883333325386
72		0.229048044593	0.931481491636	0.356075167656	0.884444415569
73		0.225283583005	0.932592606103	0.352554500103	0.885555565357
74		0.221413385537	0.933333332892	0.351875722408	0.889999985695
75		0.218518285840	0.935308650688	0.352059781551	0.888888895512
76		0.215206445367	0.935679016290	0.347945153713	0.889999985695
77		0.212072056201	0.935185189600	0.351162075996	0.885555565357
78		0.208219864302	0.938024693065	0.347182631493	0.893333315849
79		0.205964751266	0.939382712046	0.356135398149	0.885555565357
80		0.203719072320	0.939135805324	0.347544670105	0.896666646004
81		0.201061355847	0.939382707631	0.359305769205	0.884444415569
82		0.199949882097	0.939135776626	0.346557468176	0.892222225666
83		0.197875148720	0.939876554189	0.350525885820	0.884444415569
84		0.192019331786	0.942839494458	0.346395611763	0.893333315849
85		0.187945522092	0.943827158875	0.350252389908	0.886666655540
86		0.185992502504	0.944691355582	0.343556165695	0.895555555820
87		0.183201499007	0.945925926721	0.348980844021	0.893333315849
88		0.180008908665	0.946543210083	0.341648131609	0.893333315849
89		0.177460162176	0.947530867877	0.347942918539	0.889999985695
90		0.175774179675	0.949259259083	0.344887018204	0.894444465637
91		0.173535377891	0.948271596873	0.344768643379	0.895555555820
92		0.171725469055	0.949135793580	0.346496492624	0.892222225666
93		0.168495254936	0.950740743566	0.344813227654	0.895555555820
94		0.164988941065	0.950617278064	0.348236739635	0.895555555820
95		0.163196099577	0.952345691345	0.343676120043	0.895555555820
96		0.160070675391	0.953580255862	0.345487803221	0.893333315849
97		0.158140106334	0.953703710326	0.345487147570	0.896666646004
98		0.155956708171	0.954814796095	0.346705436707	0.891111135483
99		0.153541523549	0.956049382687	0.348088651896	0.895555555820
100		0.151296382701	0.956666659426	0.345423400402	0.895555555820

max_length = 200
min_count=2, size=270, iter=10, sg=1, workers=10

Embedding(input_dim=len(word_index)+1, output_dim=word2Vec.vector_size, weights=[embedding_matrix], input_length=max_length, trainable=False)
GRU(16)
Dense(100, activation='relu')
Dense(100, activation='relu')
Dense(units=10, activation='softmax')

optimizer='adam'

model.fit(x=train_x, y=train_y, epochs=epochs, validation_split=.1, batch_size=3000, verbose=1)
