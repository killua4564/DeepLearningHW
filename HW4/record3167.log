result	10.50458

index	loss			mse					val_loss		val_mse
1		11.471155492086	3151.591268872458	11.377340846592	3257.478346082899
2		11.047512258802	3066.473542712984	11.132628069984	3260.560612431279
3		10.775473957970	3013.876082163009	11.131535989267	3202.164184570312
4		10.469491466643	2960.976851205977	11.112247714290	3220.793547453704
5		10.032734447055	2898.705026777964	11.152825655761	3176.742693865741
6		9.495491323017	2814.895695762029	11.200247746927	3202.026936848958
7		9.006587263138	2720.635420541915	11.344070222643	3115.306762695312
8		8.491747144669	2595.212714301215	11.347922784311	3109.175372088397
9		8.028826448652	2462.490937248109	11.349278909189	3108.838715729890
10		7.568324792953	2297.339700365823	11.516265692534	3065.107745135272

max_length=300
min_count=2, size=270, iter=10, sg=1, workers=10

inputs = Input(shape=(max_length,))
embedding = Embedding(name="embedding", input_dim=len(word_index)+1, output_dim=word2Vec.vector_size, weights=[embedding_matrix], input_length=max_length)(inputs)
maxPooling1D = MaxPooling1D(name="maxPooling1D", pool_size=3, strides=1, padding='same')(embedding)
conv1D_1 = Conv1D(name="conv1D_1", filters=128, kernel_size=1, strides=1, kernel_initializer=initializers.glorot_uniform(seed=1), padding='same', activation='relu')(maxPooling1D)
conv1D_5 = Conv1D(name="conv1D_5", filters=128, kernel_size=5, strides=5, kernel_initializer=initializers.glorot_uniform(seed=1), padding='same', activation='relu')(embedding)
concatenate = concatenate(name="concatenate", inputs=[conv1D_1, conv1D_5], axis=1)
averagePooling1D = AveragePooling1D(name="averagePooling1D", pool_size=5, strides=3)(concatenate)
flatten = Flatten(name="flatten")(averagePooling1D)
dense = Dense(name="dense", output_dim=64, kernel_initializer=initializers.glorot_uniform(seed=1), activation='relu')(flatten)
dropout = Dropout(name="dropout", rate=.3)(dense)
output = Dense(output_dim=2, kernel_initializer=initializers.glorot_uniform(seed=1), activation='relu')(dropout)
model = Model(inputs=inputs, outputs=output)
model.compile(loss='mae', optimizer=optimizers.Adam(lr=.001), metrics=['mse'])
model.summary()

epochs = 10
callback = model.fit(x=train_x, y=train_y, epochs=epochs, validation_split=.3, batch_size=1000, verbose=1).history
